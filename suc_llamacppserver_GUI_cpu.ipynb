{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-jdrwDvfhVH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/build.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pfzaid8fnau",
        "outputId": "ac9ee115-87b3-45f9-e813-7f011c67bdc4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/build.zip\n",
            "   creating: content/llama.cpp/build/\n",
            "   creating: content/llama.cpp/build/Testing/\n",
            "   creating: content/llama.cpp/build/Testing/Temporary/\n",
            "  inflating: content/llama.cpp/build/DartConfiguration.tcl  \n",
            "  inflating: content/llama.cpp/build/CMakeCache.txt  \n",
            "  inflating: content/llama.cpp/build/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/CMakeFiles/FindOpenMP/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/FindOpenMP/ompver_C.bin  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/FindOpenMP/ompver_CXX.bin  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/Continuous.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Continuous.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Continuous.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/Continuous.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Continuous.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Continuous.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Continuous.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/3.31.6/\n",
            "   creating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdCXX/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdCXX/a.out  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdCXX/CMakeCXXCompilerId.cpp  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdCXX/tmp/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeSystem.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeCXXCompiler.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeDetermineCompilerABI_CXX.bin  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeCCompiler.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeDetermineCompilerABI_C.bin  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdC/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdC/a.out  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdC/CMakeCCompilerId.c  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdC/tmp/\n",
            "   creating: content/llama.cpp/build/CMakeFiles/CMakeScratch/\n",
            "   creating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/cmake.check_cache  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/pkgRedirects/\n",
            "   creating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/TargetDirectories.txt  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/build.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/progress.marks  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/CMakeRuleHashes.txt  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/Experimental.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Experimental.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Experimental.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/Experimental.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Experimental.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Experimental.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Experimental.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Makefile2  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/Nightly.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Nightly.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Nightly.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/Nightly.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Nightly.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Nightly.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Nightly.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/CMakeConfigureLog.yaml  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/Makefile.cmake  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/build.make  \n",
            "   creating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/\n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/compiler_depend.make  \n",
            " extracting: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/Makefile  \n",
            "   creating: content/llama.cpp/build/tests/\n",
            "  inflating: content/llama.cpp/build/tests/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/test-rope.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/test-rope.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/test-log.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/test-log.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/build.make  \n",
            " extracting: content/llama.cpp/build/tests/CMakeFiles/progress.marks  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o.d  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/test-c.c.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/test-c.c.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/build.make  \n",
            "   creating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/\n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/test-chat.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/get-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/get-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/test-chat.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/tests/Makefile  \n",
            "  inflating: content/llama.cpp/build/tests/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/common/\n",
            "  inflating: content/llama.cpp/build/common/libcommon.a  \n",
            "   creating: content/llama.cpp/build/common/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/\n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/build-info.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/build-info.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/build.make  \n",
            " extracting: content/llama.cpp/build/common/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/common/CMakeFiles/common.dir/\n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/log.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/chat.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/common.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/llguidance.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/chat.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/console.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/log.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/console.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/sampling.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/sampling.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/speculative.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/speculative.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/arg.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/arg.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/ngram-cache.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/llguidance.cpp.o  \n",
            " extracting: content/llama.cpp/build/common/CMakeFiles/common.dir/cmake_clean_target.cmake  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/common/CMakeFiles/common.dir/common.cpp.o  \n",
            "  inflating: content/llama.cpp/build/common/Makefile  \n",
            "  inflating: content/llama.cpp/build/common/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/llama.pc  \n",
            "   creating: content/llama.cpp/build/examples/\n",
            "   creating: content/llama.cpp/build/examples/tokenize/\n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/tokenize/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/tokenize/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/\n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/tokenize/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/llava/\n",
            "  inflating: content/llama.cpp/build/examples/llava/libllava_static.a  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/llava.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/clip.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/gemma3-cli.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-gemma3-cli.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/depend.make  \n",
            " extracting: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/depend.make  \n",
            " extracting: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_shared.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/llava/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/clip.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/mtmd.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/clip.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/mtmd.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/depend.make  \n",
            " extracting: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/cmake_clean.cmake  \n",
            " extracting: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/cmake_clean_target.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/depend.make  \n",
            " extracting: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/cmake_clean.cmake  \n",
            " extracting: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/cmake_clean_target.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/mtmd_static.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/llava/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/llava/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llava/libmtmd_static.a  \n",
            "   creating: content/llama.cpp/build/examples/speculative-simple/\n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/\n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/speculative-simple/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/quantize/\n",
            "  inflating: content/llama.cpp/build/examples/quantize/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/quantize/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/quantize/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/\n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/quantize/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gguf/\n",
            "  inflating: content/llama.cpp/build/examples/gguf/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gguf/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/\n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/gguf/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/gguf/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gguf-split/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/gguf-split/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-split/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gritlm/\n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gritlm/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/\n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/gritlm/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/gritlm/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/server/\n",
            "  inflating: content/llama.cpp/build/examples/server/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/server/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/server/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/\n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/server.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/server.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/server/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/server/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/server/loading.html.hpp  \n",
            "  inflating: content/llama.cpp/build/examples/server/index.html.gz.hpp  \n",
            "   creating: content/llama.cpp/build/examples/infill/\n",
            "  inflating: content/llama.cpp/build/examples/infill/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/infill/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/\n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/infill/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/infill/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/infill/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/infill/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/quantize-stats/\n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/\n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/quantize-stats/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/save-load-state/\n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/\n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/save-load-state/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/save-load-state/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/parallel/\n",
            "  inflating: content/llama.cpp/build/examples/parallel/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/parallel/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/parallel/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/\n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/parallel/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/passkey/\n",
            "  inflating: content/llama.cpp/build/examples/passkey/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/passkey/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/passkey/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/\n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/passkey/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/Makefile  \n",
            "   creating: content/llama.cpp/build/examples/simple/\n",
            "  inflating: content/llama.cpp/build/examples/simple/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/simple/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/\n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/simple/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/simple/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/simple/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/simple/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/imatrix/\n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/imatrix/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/\n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/imatrix/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/imatrix/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gen-docs/\n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/\n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/gen-docs/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/gen-docs/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/main/\n",
            "  inflating: content/llama.cpp/build/examples/main/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/main/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/\n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/main.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/main.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/main/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/main/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/main/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/main/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/simple-chat/\n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/simple-chat/CMakeFiles/progress.marks  \n",
            "   creating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/\n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/simple-chat/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/retrieval/\n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/retrieval/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/retrieval/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/\n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/retrieval/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/tts/\n",
            "  inflating: content/llama.cpp/build/examples/tts/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/tts/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/\n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/tts/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/tts/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/tts/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/tts/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/\n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/\n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/cvector-generator/\n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/\n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/cvector-generator/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/lookup/\n",
            "  inflating: content/llama.cpp/build/examples/lookup/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/lookup/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/\n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/\n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/\n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/lookup/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/\n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/lookup/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/lookahead/\n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/lookahead/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/lookahead/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/\n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/lookahead/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/batched-bench/\n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/\n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/batched-bench/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/batched-bench/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/batched/\n",
            "  inflating: content/llama.cpp/build/examples/batched/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/batched/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/\n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/batched/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/batched/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/batched/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/batched/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/eval-callback/\n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/eval-callback/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/\n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/eval-callback/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/llama-bench/\n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/llama-bench/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/\n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/llama-bench/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/export-lora/\n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/export-lora/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/\n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/export-lora/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/export-lora/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gbnf-validator/\n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/\n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/gbnf-validator/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/perplexity/\n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/perplexity/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/examples/perplexity/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "   creating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/\n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/perplexity/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/embedding/\n",
            "  inflating: content/llama.cpp/build/examples/embedding/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/embedding/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/\n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/embedding/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/embedding/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/speculative/\n",
            "  inflating: content/llama.cpp/build/examples/speculative/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/speculative/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/\n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/speculative/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/speculative/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/run/\n",
            "  inflating: content/llama.cpp/build/examples/run/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/run/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/\n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/DependInfo.cmake  \n",
            "   creating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/\n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/run.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/run.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/run/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/run/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/run/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/run/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/depend.make  \n",
            " extracting: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/cmake_clean.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/\n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/depend.make  \n",
            " extracting: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/cmake_clean.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/\n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/build.make  \n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/depend.make  \n",
            " extracting: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/cmake_clean.cmake  \n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/\n",
            "   creating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/\n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o.d  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/build.make  \n",
            " extracting: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/Makefile  \n",
            "  inflating: content/llama.cpp/build/examples/gguf-hash/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/llama-version.cmake  \n",
            "   creating: content/llama.cpp/build/ggml/\n",
            "   creating: content/llama.cpp/build/ggml/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/ggml/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/ggml/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/Makefile  \n",
            "  inflating: content/llama.cpp/build/ggml/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/ggml-config.cmake  \n",
            "   creating: content/llama.cpp/build/ggml/src/\n",
            "   creating: content/llama.cpp/build/ggml/src/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/\n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/build.make  \n",
            "   creating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/\n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/build.make  \n",
            " extracting: content/llama.cpp/build/ggml/src/CMakeFiles/progress.marks  \n",
            "   creating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/\n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/DependInfo.cmake  \n",
            "   creating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/\n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o.d  \n",
            "   creating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/\n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o  \n",
            "   creating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/\n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/ggml/src/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/src/Makefile  \n",
            "   creating: content/llama.cpp/build/ggml/src/ggml-cpu/\n",
            "   creating: content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/src/ggml-cpu/Makefile  \n",
            "  inflating: content/llama.cpp/build/ggml/src/ggml-cpu/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/src/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/ggml/ggml-version.cmake  \n",
            "  inflating: content/llama.cpp/build/llama-config.cmake  \n",
            "   creating: content/llama.cpp/build/bin/\n",
            "  inflating: content/llama.cpp/build/bin/llama-imatrix  \n",
            "  inflating: content/llama.cpp/build/bin/test-tokenizer-1-spm  \n",
            "  inflating: content/llama.cpp/build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: content/llama.cpp/build/bin/test-rope  \n",
            "  inflating: content/llama.cpp/build/bin/test-c  \n",
            "  inflating: content/llama.cpp/build/bin/llama-qwen2vl-cli  \n",
            "  inflating: content/llama.cpp/build/bin/llama-speculative  \n",
            "  inflating: content/llama.cpp/build/bin/llama-embedding  \n",
            "  inflating: content/llama.cpp/build/bin/libggml-cpu.so  \n",
            "  inflating: content/llama.cpp/build/bin/test-tokenizer-0  \n",
            "  inflating: content/llama.cpp/build/bin/llama-save-load-state  \n",
            "  inflating: content/llama.cpp/build/bin/llama-gen-docs  \n",
            "  inflating: content/llama.cpp/build/bin/llama-cli  \n",
            "  inflating: content/llama.cpp/build/bin/llama-batched  \n",
            "  inflating: content/llama.cpp/build/bin/llama-gguf-hash  \n",
            "  inflating: content/llama.cpp/build/bin/test-sampling  \n",
            "  inflating: content/llama.cpp/build/bin/test-gguf  \n",
            "  inflating: content/llama.cpp/build/bin/llama-simple-chat  \n",
            "  inflating: content/llama.cpp/build/bin/test-autorelease  \n",
            "  inflating: content/llama.cpp/build/bin/llama-lookup  \n",
            "  inflating: content/llama.cpp/build/bin/llama-quantize  \n",
            "  inflating: content/llama.cpp/build/bin/test-backend-ops  \n",
            "  inflating: content/llama.cpp/build/bin/test-model-load-cancel  \n",
            "  inflating: content/llama.cpp/build/bin/libggml.so  \n",
            "  inflating: content/llama.cpp/build/bin/llama-batched-bench  \n",
            "  inflating: content/llama.cpp/build/bin/llama-gritlm  \n",
            "  inflating: content/llama.cpp/build/bin/test-chat  \n",
            "  inflating: content/llama.cpp/build/bin/llama-q8dot  \n",
            "  inflating: content/llama.cpp/build/bin/llama-eval-callback  \n",
            "  inflating: content/llama.cpp/build/bin/llama-llava-clip-quantize-cli  \n",
            "  inflating: content/llama.cpp/build/bin/llama-lookup-stats  \n",
            "  inflating: content/llama.cpp/build/bin/test-json-schema-to-grammar  \n",
            "  inflating: content/llama.cpp/build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: content/llama.cpp/build/bin/llama-lookahead  \n",
            "  inflating: content/llama.cpp/build/bin/llama-lookup-create  \n",
            "  inflating: content/llama.cpp/build/bin/test-grammar-parser  \n",
            "  inflating: content/llama.cpp/build/bin/llama-perplexity  \n",
            "  inflating: content/llama.cpp/build/bin/llama-minicpmv-cli  \n",
            "  inflating: content/llama.cpp/build/bin/libllava_shared.so  \n",
            "  inflating: content/llama.cpp/build/bin/llama-quantize-stats  \n",
            "  inflating: content/llama.cpp/build/bin/libmtmd_shared.so  \n",
            "  inflating: content/llama.cpp/build/bin/llama-parallel  \n",
            "  inflating: content/llama.cpp/build/bin/llama-lookup-merge  \n",
            "  inflating: content/llama.cpp/build/bin/test-log  \n",
            "  inflating: content/llama.cpp/build/bin/llama-export-lora  \n",
            "  inflating: content/llama.cpp/build/bin/libllama.so  \n",
            "  inflating: content/llama.cpp/build/bin/llama-bench  \n",
            "  inflating: content/llama.cpp/build/bin/llama-tts  \n",
            "  inflating: content/llama.cpp/build/bin/llama-gbnf-validator  \n",
            "  inflating: content/llama.cpp/build/bin/test-chat-template  \n",
            "  inflating: content/llama.cpp/build/bin/test-quantize-perf  \n",
            "  inflating: content/llama.cpp/build/bin/llama-gguf-split  \n",
            "  inflating: content/llama.cpp/build/bin/llama-infill  \n",
            "  inflating: content/llama.cpp/build/bin/libggml-base.so  \n",
            "  inflating: content/llama.cpp/build/bin/llama-gemma3-cli  \n",
            "  inflating: content/llama.cpp/build/bin/llama-vdot  \n",
            "  inflating: content/llama.cpp/build/bin/test-arg-parser  \n",
            "  inflating: content/llama.cpp/build/bin/llama-tokenize  \n",
            "  inflating: content/llama.cpp/build/bin/llama-speculative-simple  \n",
            "  inflating: content/llama.cpp/build/bin/llama-gguf  \n",
            "  inflating: content/llama.cpp/build/bin/test-llama-grammar  \n",
            "  inflating: content/llama.cpp/build/bin/llama-run  \n",
            "  inflating: content/llama.cpp/build/bin/llama-server  \n",
            "  inflating: content/llama.cpp/build/bin/llama-passkey  \n",
            "  inflating: content/llama.cpp/build/bin/test-quantize-fns  \n",
            "  inflating: content/llama.cpp/build/bin/llama-llava-cli  \n",
            "  inflating: content/llama.cpp/build/bin/llama-simple  \n",
            "  inflating: content/llama.cpp/build/bin/llama-cvector-generator  \n",
            "  inflating: content/llama.cpp/build/bin/test-barrier  \n",
            "  inflating: content/llama.cpp/build/bin/test-grammar-integration  \n",
            "  inflating: content/llama.cpp/build/bin/llama-retrieval  \n",
            "  inflating: content/llama.cpp/build/compile_commands.json  \n",
            "   creating: content/llama.cpp/build/pocs/\n",
            "  inflating: content/llama.cpp/build/pocs/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/pocs/CMakeFiles/\n",
            " extracting: content/llama.cpp/build/pocs/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/pocs/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/pocs/Makefile  \n",
            "   creating: content/llama.cpp/build/pocs/vdot/\n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CTestTestfile.cmake  \n",
            "   creating: content/llama.cpp/build/pocs/vdot/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/\n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/build.make  \n",
            "   creating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/\n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/build.make  \n",
            " extracting: content/llama.cpp/build/pocs/vdot/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/Makefile  \n",
            "  inflating: content/llama.cpp/build/pocs/vdot/cmake_install.cmake  \n",
            "  inflating: content/llama.cpp/build/pocs/cmake_install.cmake  \n",
            "   creating: content/llama.cpp/build/src/\n",
            "   creating: content/llama.cpp/build/src/CMakeFiles/\n",
            "   creating: content/llama.cpp/build/src/CMakeFiles/llama.dir/\n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-arch.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-quant.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode-data.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/compiler_depend.ts  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/compiler_depend.make  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-batch.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/depend.make  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/progress.make  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/link.txt  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/DependInfo.cmake  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-chat.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-graph.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-impl.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-io.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-memory.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/flags.make  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-context.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/cmake_clean.cmake  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-memory.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-io.cpp.o  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/build.make  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-context.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-graph.cpp.o.d  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d  \n",
            " extracting: content/llama.cpp/build/src/CMakeFiles/progress.marks  \n",
            "  inflating: content/llama.cpp/build/src/CMakeFiles/CMakeDirectoryInformation.cmake  \n",
            "  inflating: content/llama.cpp/build/src/Makefile  \n",
            "  inflating: content/llama.cpp/build/src/cmake_install.cmake  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTbdIUc-fqAn",
        "outputId": "e4fcc95d-703b-4db3-807b-5554dfd0d5ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with; for system message, use -sys\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: 0.1, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "--override-tensor, -ot <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default: penalties;dry;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq, --sampler-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default: edkypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on infinite text generation (default: disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-sys,  --system-prompt PROMPT           system prompt to use with model (if applicable, depending on chat\n",
            "                                        template)\n",
            "-sysf, --system-prompt-file FNAME       a file containing the system prompt (default: none)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: auto enabled if chat template is available)\n",
            "-no-cnv, --no-conversation              force disable conversation mode (default: false)\n",
            "-st,   --single-turn                    run conversation for a single turn only, then exit when done\n",
            "                                        will not be interactive if first turn is predefined with --prompt\n",
            "                                        (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--jinja                                 use jinja template for chat (default: disabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               reasoning format (default: deepseek; allowed values: deepseek, none)\n",
            "                                        controls whether thought tags are extracted from the response, and in\n",
            "                                        which format they're returned. 'none' leaves thoughts unparsed in\n",
            "                                        `message.content`, 'deepseek' puts them in `message.reasoning_content`\n",
            "                                        (for DeepSeek R1 & Command R7B only).\n",
            "                                        only supported for non-streamed responses\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, falcon3, gemma, gigachat, glmedge, granite,\n",
            "                                        llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4,\n",
            "                                        megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken,\n",
            "                                        mistral-v7, monarch, openchat, orion, phi3, phi4, rwkv-world, vicuna,\n",
            "                                        vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, falcon3, gemma, gigachat, glmedge, granite,\n",
            "                                        llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4,\n",
            "                                        megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken,\n",
            "                                        mistral-v7, monarch, openchat, orion, phi3, phi4, rwkv-world, vicuna,\n",
            "                                        vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     /content/llama.cpp/build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv\n",
            "\n",
            "  chat (conversation): /content/llama.cpp/build/bin/llama-cli -m your_model.gguf -sys \"You are a helpful assistant\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrDN5Kz0gdq2",
        "outputId": "d6dc948b-ce7d-42a5-df5f-f96f503619ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-16 02:52:51--  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.12, 3.165.160.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457f561dbc064f4a6413f/4cb1444f81355e47d236ada8190f0325ce46412a83f3ab62a1d63bb314592ebc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250416T025251Z&X-Amz-Expires=3600&X-Amz-Signature=6f0774e2af72fefa5e246ae2d6bec453493b747ce94f4a3aa6172342e321973d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-Q4_K_M.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1744775571&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc3NTU3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdmNTYxZGJjMDY0ZjRhNjQxM2YvNGNiMTQ0NGY4MTM1NWU0N2QyMzZhZGE4MTkwZjAzMjVjZTQ2NDEyYTgzZjNhYjYyYTFkNjNiYjMxNDU5MmViYyoifV19&Signature=R%7EEJpSOPY7VzSNmtb8NKFZBQUVWaKD0Gn7cwJaj7gmM7Glxi8w7POjtjkbvsDXlKtcljDTnu8KreYVuWK55pm%7E4Hu9%7EEgwJdqKI3YTLVvb15d5QdfawrZjtCWpOb16zUqQA9-OXTcOTLzBeCv43I3dF0QtciuTqjGOXGyMEhIuUkVzldbQAbdA1Vw2jJc27lYaA%7EGGuavlttzYkm5S4QwtKdUVzGv9MNMn6kPlf8naTsOJ1qvsUnTKQSaejY%7EM%7EG5Rt0GuCS5AztCXiFTOQkTeZl6kc%7EIzmZKofgGpRoFG9gfgYDjdTzrMT62XCvBvvokJqzydGSGxRUWlbVfMeL9A__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-04-16 02:52:52--  https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457f561dbc064f4a6413f/4cb1444f81355e47d236ada8190f0325ce46412a83f3ab62a1d63bb314592ebc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250416%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250416T025251Z&X-Amz-Expires=3600&X-Amz-Signature=6f0774e2af72fefa5e246ae2d6bec453493b747ce94f4a3aa6172342e321973d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-Q4_K_M.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1744775571&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDc3NTU3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdmNTYxZGJjMDY0ZjRhNjQxM2YvNGNiMTQ0NGY4MTM1NWU0N2QyMzZhZGE4MTkwZjAzMjVjZTQ2NDEyYTgzZjNhYjYyYTFkNjNiYjMxNDU5MmViYyoifV19&Signature=R%7EEJpSOPY7VzSNmtb8NKFZBQUVWaKD0Gn7cwJaj7gmM7Glxi8w7POjtjkbvsDXlKtcljDTnu8KreYVuWK55pm%7E4Hu9%7EEgwJdqKI3YTLVvb15d5QdfawrZjtCWpOb16zUqQA9-OXTcOTLzBeCv43I3dF0QtciuTqjGOXGyMEhIuUkVzldbQAbdA1Vw2jJc27lYaA%7EGGuavlttzYkm5S4QwtKdUVzGv9MNMn6kPlf8naTsOJ1qvsUnTKQSaejY%7EM%7EG5Rt0GuCS5AztCXiFTOQkTeZl6kc%7EIzmZKofgGpRoFG9gfgYDjdTzrMT62XCvBvvokJqzydGSGxRUWlbVfMeL9A__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.63, 18.238.217.88, 18.238.217.126, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2019377696 (1.9G)\n",
            "Saving to: ‘Llama-3.2-3B-Instruct-Q4_K_M.gguf’\n",
            "\n",
            "Llama-3.2-3B-Instru 100%[===================>]   1.88G  76.8MB/s    in 26s     \n",
            "\n",
            "2025-04-16 02:53:17 (75.0 MB/s) - ‘Llama-3.2-3B-Instruct-Q4_K_M.gguf’ saved [2019377696/2019377696]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-cli -m /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf -p \"What is a car?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6aivWfDg3cm",
        "outputId": "e3f0b9b5-ea0b-4388-8986-0fdb0ab87611"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...\n",
            "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q4_K:  168 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.7999 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 24\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 3\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 500000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.21 B\n",
            "print_info: general.name     = Llama 3.2 3B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128009 '<|eot_id|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: EOM token        = 128008 '<|eom_id|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 128008 '<|eom_id|>'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:  CPU_AARCH64 model buffer size =  1299.38 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1918.35 MiB\n",
            "....................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 500000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.49 MiB\n",
            "init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
            "init:        CPU KV buffer size =   448.00 MiB\n",
            "llama_context: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
            "llama_context:        CPU compute buffer size =   256.50 MiB\n",
            "llama_context: graph nodes  = 958\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "main: chat template is available, enabling conversation mode (disable it with -no-cnv)\n",
            "*** User-specified prompt will pre-start conversation, did you mean to set --system-prompt (-sys) instead?\n",
            "main: chat template example:\n",
            "<|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "main: interactive mode on.\n",
            "sampler seed: 4082504955\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to the AI.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            " - Not using system message. To change it, set a different value via -sys PROMPT\n",
            "\n",
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 16 Apr 2025\n",
            "\n",
            "user\n",
            "\n",
            "What is a car?assistant\n",
            "\n",
            "A car, also known as an automobile or motor vehicle, is a road vehicle that is powered by an internal combustion engine or an electric motor. It is typically designed to transport people or goods on paved roads.\n",
            "\n",
            "Cars typically have four wheels, a body that contains the engine, transmission, and other essential systems, and a frame that provides support and structure. They usually have a driver's seat, a passenger compartment, and sometimes a trunk or cargo area for storing goods.\n",
            "\n",
            "Cars come in a variety of shapes, sizes, and styles, including\n",
            "llama_perf_sampler_print:    sampling time =      21.30 ms /   148 runs   (    0.14 ms per token,  6948.03 tokens per second)\n",
            "llama_perf_context_print:        load time =    4228.79 ms\n",
            "llama_perf_context_print: prompt eval time =    5450.86 ms /    40 tokens (  136.27 ms per token,     7.34 tokens per second)\n",
            "llama_perf_context_print:        eval time =   41737.97 ms /   107 runs   (  390.07 ms per token,     2.56 tokens per second)\n",
            "llama_perf_context_print:       total time =   47354.18 ms /   147 tokens\n",
            "Interrupted by user\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/build/bin/llama-server -m /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmNC3SdXhIXW",
        "outputId": "a567d0df-a603-4f6d-8135-67d8af20278d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "main: binding port with default address family\n",
            "main: couldn't bind HTTP server socket, hostname: 127.0.0.1, port: 8080\n",
            "srv    operator(): operator(): cleaning up before exit...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz-dIMzjhxz3",
        "outputId": "9323b746-7fd6-40ca-fd41-373fb7cde737"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.4-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "\n",
        "ngrok_token = \"2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d\"\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Start ngrok in a separate thread to avoid blocking\n",
        "def start_ngrok():\n",
        "    public_url = ngrok.connect(8080).public_url\n",
        "    print(f\"🚀 Ngrok Tunnel Open: {public_url}\")\n",
        "\n",
        "ngrok_thread = threading.Thread(target=start_ngrok)\n",
        "ngrok_thread.start()\n",
        "\n",
        "# Wait for ngrok to start (optional)\n",
        "time.sleep(5)\n",
        "\n",
        "# Execute your node.js script\n",
        "!/content/llama.cpp/build/bin/llama-server -m /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS02_Oihh0o2",
        "outputId": "2b68518c-f933-4ce4-fede-15f55581b8a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Ngrok Tunnel Open: https://fba3-35-197-34-232.ngrok-free.app\n",
            "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "main: binding port with default address family\n",
            "main: couldn't bind HTTP server socket, hostname: 127.0.0.1, port: 8080\n",
            "srv    operator(): operator(): cleaning up before exit...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "\n",
        "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d\n",
        "\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "\n",
        "ngrok_token = \"2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d\"\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Start ngrok in a separate thread to avoid blocking\n",
        "def start_ngrok():\n",
        "    public_url = ngrok.connect(3000).public_url\n",
        "    print(f\"🚀 Ngrok Tunnel Open: {public_url}\")\n",
        "\n",
        "ngrok_thread = threading.Thread(target=start_ngrok)\n",
        "ngrok_thread.start()\n",
        "\n",
        "# Wait for ngrok to start (optional)\n",
        "time.sleep(5)\n",
        "\n",
        "# Execute your node.js script\n",
        "!node hello-world.js"
      ],
      "metadata": {
        "id": "aodhvtUshSqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!/content/llama.cpp/build/bin/llama-server -m /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc1aYxmeiY5Q",
        "outputId": "7570f0da-7100-4e75-d5bc-10d24353047d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "main: binding port with default address family\n",
            "main: couldn't bind HTTP server socket, hostname: 127.0.0.1, port: 8080\n",
            "srv    operator(): operator(): cleaning up before exit...\n"
          ]
        }
      ]
    },
    {
      "source": [
        "public_url = ngrok.connect(8080).public_url  # Replace 3000 with your chosen port\n",
        "print(f\"🚀 Ngrok Tunnel Open: {public_url}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuVEQiBoiZSH",
        "outputId": "b640f66d-7d5d-4488-f84e-6f047f3c8d5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Ngrok Tunnel Open: https://a051-35-197-34-232.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wF0bIYdOjLxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "from pyngrok import ngrok, conf\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# ----- إعدادات -----\n",
        "NGROK_TOKEN = \"2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d\" # استبدله بالتوكن الخاص بك أو استخدم Colab Secrets\n",
        "MODEL_PATH = \"/content/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"        # مسار الموديل الذي تم تحميله\n",
        "LLAMA_SERVER_PATH = \"/content/llama.cpp/build/bin/llama-server\"  # مسار الملف التنفيذي للخادم\n",
        "PORT = 8080                                                     # المنفذ الذي سيعمل عليه الخادم (الافتراضي لـ llama-server)\n",
        "HOST = \"0.0.0.0\"                                                # للاستماع على جميع الواجهات\n",
        "\n",
        "# ----- إعداد Ngrok -----\n",
        "# يفضل استخدام Colab Secrets:\n",
        "# from google.colab import userdata\n",
        "# NGROK_TOKEN = userdata.get('NGROK_AUTH_TOKEN') # اسم السيكرت يجب أن يكون NGROK_AUTH_TOKEN\n",
        "\n",
        "if not NGROK_TOKEN:\n",
        "    print(\"⚠️ يرجى تعيين توكن Ngrok الخاص بك في متغير NGROK_TOKEN أو عبر Colab Secrets باسم 'NGROK_AUTH_TOKEN'\")\n",
        "else:\n",
        "    try:\n",
        "        conf.get_default().auth_token = NGROK_TOKEN\n",
        "        print(\"🔑 تم تعيين توكن المصادقة لـ Ngrok.\")\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ في إعداد توكن Ngrok: {e}\")\n",
        "        # يمكنك إيقاف التنفيذ هنا إذا كان التوكن ضرورياً\n",
        "        # raise SystemExit()\n",
        "\n",
        "# متغير لتخزين رابط Ngrok العام\n",
        "public_url = None\n",
        "ngrok_thread_started = threading.Event() # للإشارة عند بدء تشغيل نفق ngrok\n",
        "\n",
        "# وظيفة لبدء Ngrok في thread منفصل\n",
        "def start_ngrok(port):\n",
        "    global public_url\n",
        "    try:\n",
        "        # قطع أي أنفاق قديمة قد تكون نشطة لنفس التوكن\n",
        "        for tunnel in ngrok.get_tunnels():\n",
        "            ngrok.disconnect(tunnel.public_url)\n",
        "            print(f\"🔌 تم قطع نفق Ngrok قديم: {tunnel.public_url}\")\n",
        "        # بدء نفق جديد\n",
        "        tunnel = ngrok.connect(port, \"http\")\n",
        "        public_url = tunnel.public_url\n",
        "        print(f\"🚀 نفق Ngrok يعمل: {public_url}\")\n",
        "        print(f\"🔗 يمكن الوصول للخادم عبر هذا الرابط.\")\n",
        "        ngrok_thread_started.set() # إرسال إشارة بأن النفق جاهز\n",
        "        # إبقاء الـ thread يعمل حتى يتم إيقاف البرنامج الرئيسي\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ خطأ أثناء تشغيل Ngrok: {e}\")\n",
        "        public_url = f\"Error: {e}\"\n",
        "        ngrok_thread_started.set() # إرسال إشارة حتى لو حدث خطأ لتجنب الانتظار للأبد\n",
        "\n",
        "# ----- تشغيل Ngrok -----\n",
        "if NGROK_TOKEN: # فقط إذا كان التوكن موجودًا\n",
        "    print(\"⏳ بدء تشغيل Ngrok في الخلفية...\")\n",
        "    ngrok_thread = threading.Thread(target=start_ngrok, args=(PORT,))\n",
        "    ngrok_thread.daemon = True # اجعل الـ thread يموت عند انتهاء البرنامج الرئيسي\n",
        "    ngrok_thread.start()\n",
        "\n",
        "    # الانتظار حتى يبدأ Ngrok ويحصل على الرابط أو يحدث خطأ\n",
        "    print(\"⏳ الانتظار لبدء نفق Ngrok...\")\n",
        "    started = ngrok_thread_started.wait(timeout=15) # انتظر 15 ثانية كحد أقصى\n",
        "\n",
        "    if not started:\n",
        "        print(\"⚠️ استغرق Ngrok وقتًا طويلاً للبدء أو فشل. قد لا يعمل الخادم بشكل صحيح عبر الإنترنت.\")\n",
        "    elif public_url and \"Error\" in public_url:\n",
        "         print(f\"❌ فشل Ngrok في البدء بشكل صحيح: {public_url}\")\n",
        "         # يمكنك اختيار إيقاف الخلية هنا إذا أردت\n",
        "         # raise SystemExit(\"فشل Ngrok\")\n",
        "    else:\n",
        "        print(\"✅ Ngrok جاهز.\")\n",
        "\n",
        "# ----- تشغيل خادم Llama -----\n",
        "# تأكد من أن الملفات موجودة قبل التشغيل\n",
        "if not os.path.exists(LLAMA_SERVER_PATH):\n",
        "    print(f\"❌ خطأ: لم يتم العثور على الملف التنفيذي للخادم في {LLAMA_SERVER_PATH}\")\n",
        "    print(\"❓ هل قمت بتشغيل خطوات البناء في الخطوة 1؟\")\n",
        "elif not os.path.exists(MODEL_PATH):\n",
        "    print(f\"❌ خطأ: لم يتم العثور على ملف الموديل في {MODEL_PATH}\")\n",
        "    print(\"❓ هل قمت بتحميل الموديل إلى المسار الصحيح؟\")\n",
        "else:\n",
        "    print(f\"\\n🚀 بدء تشغيل llama-server على المنفذ {PORT}...\")\n",
        "    print(f\"   باستخدام الموديل: {MODEL_PATH}\")\n",
        "\n",
        "    # بناء أمر التشغيل\n",
        "    # أضف وسائط أخرى حسب الحاجة، مثل:\n",
        "    # -c 2048 (حجم السياق)\n",
        "    # -ngl 35 (عدد الطبقات لتشغيلها على GPU - إذا تم بناء llama.cpp مع دعم GPU)\n",
        "    # --host 0.0.0.0 (للاستماع على جميع الواجهات)\n",
        "    # --port 8080 (لتحديد المنفذ)\n",
        "    command = f\"{LLAMA_SERVER_PATH} -m {MODEL_PATH} --host {HOST} --port {PORT} -c 2048\" # مثال مع حجم سياق\n",
        "\n",
        "    # أضف خيار GPU إذا كان متاحًا (اضبط العدد حسب ذاكرة الـ VRAM لديك)\n",
        "    # command += \" -ngl 35\" # مثال لـ 35 طبقة على GPU\n",
        "\n",
        "    print(f\"\\n✨ الأمر الذي سيتم تنفيذه:\\n   {command}\\n\")\n",
        "\n",
        "    # تنفيذ الأمر (سيوقف هذا الخلية حتى توقف الخادم يدوياً)\n",
        "    !{command}\n",
        "\n",
        "    # الكود هنا لن يتم تنفيذه إلا بعد إيقاف الخادم (مثل الضغط على زر الإيقاف في Colab)\n",
        "    print(\"\\n🛑 تم إيقاف خادم Llama.\")\n",
        "\n",
        "    # إيقاف Ngrok عند إيقاف الخادم (اختياري، الـ thread سينتهي تلقائياً إذا كان daemon)\n",
        "    try:\n",
        "        ngrok.disconnect(public_url)\n",
        "        print(\"🔌 تم قطع اتصال نفق Ngrok.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ لم يتمكن من قطع اتصال Ngrok أو أنه قُطع بالفعل: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1wwYzfUjLuP",
        "outputId": "24410e72-a0ed-4630-c326-b1b92929e3f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 تم تعيين توكن المصادقة لـ Ngrok.\n",
            "⏳ بدء تشغيل Ngrok في الخلفية...\n",
            "⏳ الانتظار لبدء نفق Ngrok...\n",
            "🚀 نفق Ngrok يعمل: https://0d1d-35-197-34-232.ngrok-free.app\n",
            "🔗 يمكن الوصول للخادم عبر هذا الرابط.\n",
            "✅ Ngrok جاهز.\n",
            "\n",
            "🚀 بدء تشغيل llama-server على المنفذ 8080...\n",
            "   باستخدام الموديل: /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n",
            "\n",
            "✨ الأمر الذي سيتم تنفيذه:\n",
            "   /content/llama.cpp/build/bin/llama-server -m /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf --host 0.0.0.0 --port 8080 -c 2048\n",
            "\n",
            "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "main: binding port with default address family\n",
            "main: couldn't bind HTTP server socket, hostname: 0.0.0.0, port: 8080\n",
            "srv    operator(): operator(): cleaning up before exit...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-04-16T03:03:30+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8080-8ae8a27c-8059-43e9-b896-058e023a40c3 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🛑 تم إيقاف خادم Llama.\n",
            "🔌 تم قطع اتصال نفق Ngrok.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال"
      ],
      "metadata": {
        "id": "U9YKV8vXkGfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "from pyngrok import ngrok, conf\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# ----- إعدادات -----\n",
        "NGROK_TOKEN = \"2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d\"\n",
        "MODEL_PATH = \"/content/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
        "LLAMA_SERVER_PATH = \"/content/llama.cpp/build/bin/llama-server\"\n",
        "PORT = 8081  # <---- تم تغيير المنفذ هنا\n",
        "HOST = \"0.0.0.0\"\n",
        "\n",
        "# ----- إعداد Ngrok -----\n",
        "if not NGROK_TOKEN:\n",
        "    print(\"⚠️ يرجى تعيين توكن Ngrok الخاص بك...\")\n",
        "else:\n",
        "    try:\n",
        "        conf.get_default().auth_token = NGROK_TOKEN\n",
        "        print(\"🔑 تم تعيين توكن المصادقة لـ Ngrok.\")\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ في إعداد توكن Ngrok: {e}\")\n",
        "\n",
        "public_url = None\n",
        "ngrok_thread_started = threading.Event()\n",
        "\n",
        "def start_ngrok(port_to_forward): # تغيير اسم المتغير لتوضيح أنه المنفذ المراد توجيهه\n",
        "    global public_url\n",
        "    try:\n",
        "        for tunnel in ngrok.get_tunnels():\n",
        "            ngrok.disconnect(tunnel.public_url)\n",
        "            print(f\"🔌 تم قطع نفق Ngrok قديم: {tunnel.public_url}\")\n",
        "        tunnel = ngrok.connect(port_to_forward, \"http\") # <---- استخدام المنفذ الجديد هنا\n",
        "        public_url = tunnel.public_url\n",
        "        print(f\"🚀 نفق Ngrok يعمل على المنفذ {port_to_forward}: {public_url}\")\n",
        "        print(f\"🔗 يمكن الوصول للخادم عبر هذا الرابط.\")\n",
        "        ngrok_thread_started.set()\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ خطأ أثناء تشغيل Ngrok: {e}\")\n",
        "        public_url = f\"Error: {e}\"\n",
        "        ngrok_thread_started.set()\n",
        "\n",
        "# ----- تشغيل Ngrok -----\n",
        "if NGROK_TOKEN:\n",
        "    print(\"⏳ بدء تشغيل Ngrok في الخلفية...\")\n",
        "    ngrok_thread = threading.Thread(target=start_ngrok, args=(PORT,)) # تمرير المنفذ الجديد\n",
        "    ngrok_thread.daemon = True\n",
        "    ngrok_thread.start()\n",
        "    print(\"⏳ الانتظار لبدء نفق Ngrok...\")\n",
        "    started = ngrok_thread_started.wait(timeout=15)\n",
        "    if not started:\n",
        "        print(\"⚠️ استغرق Ngrok وقتًا طويلاً للبدء أو فشل.\")\n",
        "    elif public_url and \"Error\" in public_url:\n",
        "         print(f\"❌ فشل Ngrok في البدء بشكل صحيح: {public_url}\")\n",
        "    else:\n",
        "        print(\"✅ Ngrok جاهز.\")\n",
        "\n",
        "# ----- تشغيل خادم Llama -----\n",
        "if not os.path.exists(LLAMA_SERVER_PATH):\n",
        "    print(f\"❌ خطأ: لم يتم العثور على الملف التنفيذي للخادم في {LLAMA_SERVER_PATH}\")\n",
        "elif not os.path.exists(MODEL_PATH):\n",
        "    print(f\"❌ خطأ: لم يتم العثور على ملف الموديل في {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"\\n🚀 بدء تشغيل llama-server على المنفذ {PORT}...\") # طباعة المنفذ الجديد\n",
        "    print(f\"   باستخدام الموديل: {MODEL_PATH}\")\n",
        "\n",
        "    command = f\"{LLAMA_SERVER_PATH} -m {MODEL_PATH} --host {HOST} --port {PORT} -c 2048\" # <---- استخدام المنفذ الجديد هنا\n",
        "\n",
        "    print(f\"\\n✨ الأمر الذي سيتم تنفيذه:\\n   {command}\\n\")\n",
        "    !{command}\n",
        "    print(\"\\n🛑 تم إيقاف خادم Llama.\")\n",
        "    try:\n",
        "        if public_url and not \"Error\" in public_url:\n",
        "             ngrok.disconnect(public_url)\n",
        "             print(\"🔌 تم قطع اتصال نفق Ngrok.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ لم يتمكن من قطع اتصال Ngrok أو أنه قُطع بالفعل: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLth0XhCjotg",
        "outputId": "2cd94155-fcba-46d9-a51a-17b36e669370"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔑 تم تعيين توكن المصادقة لـ Ngrok.\n",
            "⏳ بدء تشغيل Ngrok في الخلفية...\n",
            "⏳ الانتظار لبدء نفق Ngrok...\n",
            "🚀 نفق Ngrok يعمل على المنفذ 8081: https://b33a-35-197-34-232.ngrok-free.app\n",
            "🔗 يمكن الوصول للخادم عبر هذا الرابط.\n",
            "✅ Ngrok جاهز.\n",
            "\n",
            "🚀 بدء تشغيل llama-server على المنفذ 8081...\n",
            "   باستخدام الموديل: /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n",
            "\n",
            "✨ الأمر الذي سيتم تنفيذه:\n",
            "   /content/llama.cpp/build/bin/llama-server -m /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf --host 0.0.0.0 --port 8081 -c 2048\n",
            "\n",
            "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "\n",
            "main: binding port with default address family\n",
            "main: HTTP server is listening, hostname: 0.0.0.0, port: 8081, http threads: 3\n",
            "main: loading model\n",
            "srv    load_model: loading model '/content/Llama-3.2-3B-Instruct-Q4_K_M.gguf'\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
            "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
            "llama_model_loader: - kv   9:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...\n",
            "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q4_K:  168 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
            "load: special tokens cache size = 256\n",
            "load: token to piece cache size = 0.7999 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 24\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 3\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 500000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.21 B\n",
            "print_info: general.name     = Llama 3.2 3B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 128256\n",
            "print_info: n_merges         = 280147\n",
            "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
            "print_info: EOS token        = 128009 '<|eot_id|>'\n",
            "print_info: EOT token        = 128009 '<|eot_id|>'\n",
            "print_info: EOM token        = 128008 '<|eom_id|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 128008 '<|eom_id|>'\n",
            "print_info: EOG token        = 128009 '<|eot_id|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors:  CPU_AARCH64 model buffer size =  1299.38 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1918.35 MiB\n",
            "....................................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 2048\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 500000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context:        CPU  output buffer size =     0.49 MiB\n",
            "init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
            "init:        CPU KV buffer size =   224.00 MiB\n",
            "llama_context: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
            "llama_context:        CPU compute buffer size =   256.50 MiB\n",
            "llama_context: graph nodes  = 958\n",
            "llama_context: graph splits = 1\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "srv          init: initializing slots, n_slots = 1\n",
            "slot         init: id  0 | task -1 | new slot n_ctx_slot = 2048\n",
            "main: model loaded\n",
            "main: chat template, chat_template: {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- if strftime_now is defined %}\n",
            "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
            "    {%- else %}\n",
            "        {%- set date_string = \"26 Jul 2024\" %}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "        {{- '\"parameters\": ' }}\n",
            "        {{- tool_call.arguments | tojson }}\n",
            "        {{- \"}\" }}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            ", example_format: '<|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "'\n",
            "main: server is listening on http://0.0.0.0:8081 - starting the main loop\n",
            "srv  update_slots: all slots are idle\n",
            "srv  log_server_r: request: GET / 127.0.0.1 200\n",
            "srv  log_server_r: request: GET /favicon.ico 127.0.0.1 404\n",
            "srv  params_from_: Chat format: Content-only\n",
            "slot launch_slot_: id  0 | task 0 | processing task\n",
            "slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 2048, n_keep = 0, n_prompt_tokens = 22\n",
            "slot update_slots: id  0 | task 0 | kv cache rm [0, end)\n",
            "slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 22, n_tokens = 22, progress = 1.000000\n",
            "slot update_slots: id  0 | task 0 | prompt done, n_past = 22, n_tokens = 22\n",
            "slot      release: id  0 | task 0 | stop processing: n_past = 31, truncated = 0\n",
            "slot print_timing: id  0 | task 0 | \n",
            "prompt eval time =    3500.99 ms /    22 tokens (  159.14 ms per token,     6.28 tokens per second)\n",
            "       eval time =    2978.29 ms /    10 tokens (  297.83 ms per token,     3.36 tokens per second)\n",
            "      total time =    6479.29 ms /    32 tokens\n",
            "srv  update_slots: all slots are idle\n",
            "srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200\n",
            "srv  params_from_: Chat format: Content-only\n",
            "slot launch_slot_: id  0 | task 11 | processing task\n",
            "slot update_slots: id  0 | task 11 | new prompt, n_ctx_slot = 2048, n_keep = 0, n_prompt_tokens = 48\n",
            "slot update_slots: id  0 | task 11 | kv cache rm [31, end)\n",
            "slot update_slots: id  0 | task 11 | prompt processing progress, n_past = 48, n_tokens = 17, progress = 0.354167\n",
            "slot update_slots: id  0 | task 11 | prompt done, n_past = 48, n_tokens = 17\n",
            "srv  cancel_tasks: cancel task, id_task = 11\n",
            "srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200\n",
            "slot      release: id  0 | task 11 | stop processing: n_past = 420, truncated = 0\n",
            "srv  update_slots: all slots are idle\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-04-16T03:10:30+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8081-854c2715-7d53-416f-86d6-80270631379a acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-04-16T03:10:30+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8081-854c2715-7d53-416f-86d6-80270631379a err=\"failed to start tunnel: session closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "srv    operator(): operator(): cleaning up before exit...\n",
            "Received second interrupt, terminating immediately.\n",
            "\n",
            "🛑 تم إيقاف خادم Llama.\n",
            "🔌 تم قطع اتصال نفق Ngrok.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔑 تم تعيين توكن المصادقة لـ Ngrok.\n",
        "⏳ بدء تشغيل Ngrok في الخلفية...\n",
        "⏳ الانتظار لبدء نفق Ngrok...\n",
        "🚀 نفق Ngrok يعمل على المنفذ 8081: https://b33a-35-197-34-232.ngrok-free.app\n",
        "🔗 يمكن الوصول للخادم عبر هذا الرابط.\n",
        "✅ Ngrok جاهز.\n",
        "\n",
        "🚀 بدء تشغيل llama-server على المنفذ 8081...\n",
        "   باستخدام الموديل: /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n",
        "\n",
        "✨ الأمر الذي سيتم تنفيذه:\n",
        "   /content/llama.cpp/build/bin/llama-server -m /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf --host 0.0.0.0 --port 8081 -c 2048\n",
        "\n",
        "build: 5142 (80f19b41) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
        "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
        "\n",
        "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 |\n",
        "\n",
        "main: binding port with default address family\n",
        "main: HTTP server is listening, hostname: 0.0.0.0, port: 8081, http threads: 3\n",
        "main: loading model\n",
        "srv    load_model: loading model '/content/Llama-3.2-3B-Instruct-Q4_K_M.gguf'\n",
        "llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from /content/Llama-3.2-3B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
        "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
        "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
        "llama_model_loader: - kv   1:                               general.type str              = model\n",
        "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
        "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
        "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
        "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
        "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
        "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
        "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
        "llama_model_loader: - kv   9:                          llama.block_count u32              = 28\n",
        "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
        "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\n",
        "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
        "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\n",
        "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
        "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
        "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
        "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\n",
        "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\n",
        "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
        "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
        "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\n",
        "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
        "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
        "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
        "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
        "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
        "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
        "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
        "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
        "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
        "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-GGU...\n",
        "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
        "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196\n",
        "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 125\n",
        "llama_model_loader: - type  f32:   58 tensors\n",
        "llama_model_loader: - type q4_K:  168 tensors\n",
        "llama_model_loader: - type q6_K:   29 tensors\n",
        "print_info: file format = GGUF V3 (latest)\n",
        "print_info: file type   = Q4_K - Medium\n",
        "print_info: file size   = 1.87 GiB (5.01 BPW)\n",
        "load: special tokens cache size = 256\n",
        "load: token to piece cache size = 0.7999 MB\n",
        "print_info: arch             = llama\n",
        "print_info: vocab_only       = 0\n",
        "print_info: n_ctx_train      = 131072\n",
        "print_info: n_embd           = 3072\n",
        "print_info: n_layer          = 28\n",
        "print_info: n_head           = 24\n",
        "print_info: n_head_kv        = 8\n",
        "print_info: n_rot            = 128\n",
        "print_info: n_swa            = 0\n",
        "print_info: n_swa_pattern    = 1\n",
        "print_info: n_embd_head_k    = 128\n",
        "print_info: n_embd_head_v    = 128\n",
        "print_info: n_gqa            = 3\n",
        "print_info: n_embd_k_gqa     = 1024\n",
        "print_info: n_embd_v_gqa     = 1024\n",
        "print_info: f_norm_eps       = 0.0e+00\n",
        "print_info: f_norm_rms_eps   = 1.0e-05\n",
        "print_info: f_clamp_kqv      = 0.0e+00\n",
        "print_info: f_max_alibi_bias = 0.0e+00\n",
        "print_info: f_logit_scale    = 0.0e+00\n",
        "print_info: f_attn_scale     = 0.0e+00\n",
        "print_info: n_ff             = 8192\n",
        "print_info: n_expert         = 0\n",
        "print_info: n_expert_used    = 0\n",
        "print_info: causal attn      = 1\n",
        "print_info: pooling type     = 0\n",
        "print_info: rope type        = 0\n",
        "print_info: rope scaling     = linear\n",
        "print_info: freq_base_train  = 500000.0\n",
        "print_info: freq_scale_train = 1\n",
        "print_info: n_ctx_orig_yarn  = 131072\n",
        "print_info: rope_finetuned   = unknown\n",
        "print_info: ssm_d_conv       = 0\n",
        "print_info: ssm_d_inner      = 0\n",
        "print_info: ssm_d_state      = 0\n",
        "print_info: ssm_dt_rank      = 0\n",
        "print_info: ssm_dt_b_c_rms   = 0\n",
        "print_info: model type       = 3B\n",
        "print_info: model params     = 3.21 B\n",
        "print_info: general.name     = Llama 3.2 3B Instruct\n",
        "print_info: vocab type       = BPE\n",
        "print_info: n_vocab          = 128256\n",
        "print_info: n_merges         = 280147\n",
        "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
        "print_info: EOS token        = 128009 '<|eot_id|>'\n",
        "print_info: EOT token        = 128009 '<|eot_id|>'\n",
        "print_info: EOM token        = 128008 '<|eom_id|>'\n",
        "print_info: LF token         = 198 'Ċ'\n",
        "print_info: EOG token        = 128008 '<|eom_id|>'\n",
        "print_info: EOG token        = 128009 '<|eot_id|>'\n",
        "print_info: max token length = 256\n",
        "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
        "load_tensors:  CPU_AARCH64 model buffer size =  1299.38 MiB\n",
        "load_tensors:   CPU_Mapped model buffer size =  1918.35 MiB\n",
        "....................................................................................\n",
        "llama_context: constructing llama_context\n",
        "llama_context: n_seq_max     = 1\n",
        "llama_context: n_ctx         = 2048\n",
        "llama_context: n_ctx_per_seq = 2048\n",
        "llama_context: n_batch       = 2048\n",
        "llama_context: n_ubatch      = 512\n",
        "llama_context: causal_attn   = 1\n",
        "llama_context: flash_attn    = 0\n",
        "llama_context: freq_base     = 500000.0\n",
        "llama_context: freq_scale    = 1\n",
        "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
        "llama_context:        CPU  output buffer size =     0.49 MiB\n",
        "init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
        "init:        CPU KV buffer size =   224.00 MiB\n",
        "llama_context: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB\n",
        "llama_context:        CPU compute buffer size =   256.50 MiB\n",
        "llama_context: graph nodes  = 958\n",
        "llama_context: graph splits = 1\n",
        "common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048\n",
        "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
        "srv          init: initializing slots, n_slots = 1\n",
        "slot         init: id  0 | task -1 | new slot n_ctx_slot = 2048\n",
        "main: model loaded\n",
        "main: chat template, chat_template: {{- bos_token }}\n",
        "{%- if custom_tools is defined %}\n",
        "    {%- set tools = custom_tools %}\n",
        "{%- endif %}\n",
        "{%- if not tools_in_user_message is defined %}\n",
        "    {%- set tools_in_user_message = true %}\n",
        "{%- endif %}\n",
        "{%- if not date_string is defined %}\n",
        "    {%- if strftime_now is defined %}\n",
        "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
        "    {%- else %}\n",
        "        {%- set date_string = \"26 Jul 2024\" %}\n",
        "    {%- endif %}\n",
        "{%- endif %}\n",
        "{%- if not tools is defined %}\n",
        "    {%- set tools = none %}\n",
        "{%- endif %}\n",
        "\n",
        "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
        "{%- if messages[0]['role'] == 'system' %}\n",
        "    {%- set system_message = messages[0]['content']|trim %}\n",
        "    {%- set messages = messages[1:] %}\n",
        "{%- else %}\n",
        "    {%- set system_message = \"\" %}\n",
        "{%- endif %}\n",
        "\n",
        "{#- System message #}\n",
        "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
        "{%- if tools is not none %}\n",
        "    {{- \"Environment: ipython\\n\" }}\n",
        "{%- endif %}\n",
        "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
        "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
        "{%- if tools is not none and not tools_in_user_message %}\n",
        "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
        "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
        "    {{- \"Do not use variables.\\n\\n\" }}\n",
        "    {%- for t in tools %}\n",
        "        {{- t | tojson(indent=4) }}\n",
        "        {{- \"\\n\\n\" }}\n",
        "    {%- endfor %}\n",
        "{%- endif %}\n",
        "{{- system_message }}\n",
        "{{- \"<|eot_id|>\" }}\n",
        "\n",
        "{#- Custom tools are passed in a user message with some extra guidance #}\n",
        "{%- if tools_in_user_message and not tools is none %}\n",
        "    {#- Extract the first user message so we can plug it in here #}\n",
        "    {%- if messages | length != 0 %}\n",
        "        {%- set first_user_message = messages[0]['content']|trim %}\n",
        "        {%- set messages = messages[1:] %}\n",
        "    {%- else %}\n",
        "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
        "{%- endif %}\n",
        "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
        "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
        "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
        "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
        "    {{- \"Do not use variables.\\n\\n\" }}\n",
        "    {%- for t in tools %}\n",
        "        {{- t | tojson(indent=4) }}\n",
        "        {{- \"\\n\\n\" }}\n",
        "    {%- endfor %}\n",
        "    {{- first_user_message + \"<|eot_id|>\"}}\n",
        "{%- endif %}\n",
        "\n",
        "{%- for message in messages %}\n",
        "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
        "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
        "    {%- elif 'tool_calls' in message %}\n",
        "        {%- if not message.tool_calls|length == 1 %}\n",
        "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
        "        {%- endif %}\n",
        "        {%- set tool_call = message.tool_calls[0].function %}\n",
        "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
        "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
        "        {{- '\"parameters\": ' }}\n",
        "        {{- tool_call.arguments | tojson }}\n",
        "        {{- \"}\" }}\n",
        "        {{- \"<|eot_id|>\" }}\n",
        "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
        "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
        "        {%- if message.content is mapping or message.content is iterable %}\n",
        "            {{- message.content | tojson }}\n",
        "        {%- else %}\n",
        "            {{- message.content }}\n",
        "        {%- endif %}\n",
        "        {{- \"<|eot_id|>\" }}\n",
        "    {%- endif %}\n",
        "{%- endfor %}\n",
        "{%- if add_generation_prompt %}\n",
        "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
        "{%- endif %}\n",
        ", example_format: '<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "Hi there<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "How are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "'\n",
        "main: server is listening on http://0.0.0.0:8081 - starting the main loop\n",
        "srv  update_slots: all slots are idle\n",
        "srv  log_server_r: request: GET / 127.0.0.1 200\n",
        "srv  log_server_r: request: GET /favicon.ico 127.0.0.1 404\n",
        "srv  params_from_: Chat format: Content-only\n",
        "slot launch_slot_: id  0 | task 0 | processing task\n",
        "slot update_slots: id  0 | task 0 | new prompt, n_ctx_slot = 2048, n_keep = 0, n_prompt_tokens = 22\n",
        "slot update_slots: id  0 | task 0 | kv cache rm [0, end)\n",
        "slot update_slots: id  0 | task 0 | prompt processing progress, n_past = 22, n_tokens = 22, progress = 1.000000\n",
        "slot update_slots: id  0 | task 0 | prompt done, n_past = 22, n_tokens = 22\n",
        "slot      release: id  0 | task 0 | stop processing: n_past = 31, truncated = 0\n",
        "slot print_timing: id  0 | task 0 |\n",
        "prompt eval time =    3500.99 ms /    22 tokens (  159.14 ms per token,     6.28 tokens per second)\n",
        "       eval time =    2978.29 ms /    10 tokens (  297.83 ms per token,     3.36 tokens per second)\n",
        "      total time =    6479.29 ms /    32 tokens\n",
        "srv  update_slots: all slots are idle\n",
        "srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200\n",
        "srv  params_from_: Chat format: Content-only\n",
        "slot launch_slot_: id  0 | task 11 | processing task\n",
        "slot update_slots: id  0 | task 11 | new prompt, n_ctx_slot = 2048, n_keep = 0, n_prompt_tokens = 48\n",
        "slot update_slots: id  0 | task 11 | kv cache rm [31, end)\n",
        "slot update_slots: id  0 | task 11 | prompt processing progress, n_past = 48, n_tokens = 17, progress = 0.354167\n",
        "slot update_slots: id  0 | task 11 | prompt done, n_past = 48, n_tokens = 17"
      ],
      "metadata": {
        "id": "yeNdNvLpkZ3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8080"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln4HvUmGjrWW",
        "outputId": "563240df-2dfa-4297-c978-3f5419a9e2f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "node      7 root   21u  IPv6  19224      0t0  TCP *:8080 (LISTEN)\n",
            "node      7 root   26u  IPv6  25174      0t0  TCP 6d0a0afb4e88:8080->172.28.0.1:39814 (ESTABLISHED)\n",
            "node      7 root   28u  IPv6 123113      0t0  TCP 6d0a0afb4e88:8080->172.28.0.1:53682 (ESTABLISHED)\n",
            "node      7 root   30u  IPv6 122780      0t0  TCP 6d0a0afb4e88:8080->172.28.0.1:53688 (ESTABLISHED)\n"
          ]
        }
      ]
    }
  ]
}